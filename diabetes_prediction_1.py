# -*- coding: utf-8 -*-
"""Diabetes_prediction_1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mostafakhaled111/diabetes-prediction-1.4122ea3c-9906-46df-b7a5-c31089906611.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250713/auto/storage/goog4_request%26X-Goog-Date%3D20250713T035229Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dd5255889ff7570b5e3f1e0b3e7737df0276b608a629a752a5aca53a1d9f65ad1fde955ee566eacf15f84d3253b2011730356437a9e058b2ff6078f1e49ab01f72e0df6156d2f3fcbdb9046987ca0aa953fe1e37bb8f2532158c425c519eb885edcdf1a6d97a7a48383280cb4742383a0856d8866c150df3aca82f3ceec2b3777dc97b7ed70de5086cd8df15b3300a1719cbac5a4766ee9ab7b58e61491491c5e1672433f55e999488e547b190864c547b0530e49f6518e7e571d961f0e3dc96d49782c755870596ef299ffa6582a81e13df9bceb36bbc3783e8696f339935f37342b173c95b6eb088ff40639912197acf6190000720494d49fcb2128625be7ea
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
iammustafatz_diabetes_prediction_dataset_path = kagglehub.dataset_download('iammustafatz/diabetes-prediction-dataset')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("/kaggle/input/diabetes-prediction-dataset/diabetes_prediction_dataset.csv")

data

data.info()

data.duplicated().sum()

data.drop_duplicates(inplace = True)

data.duplicated().sum()

data = data.drop(columns=['gender'])

plt.figure(figsize=(6, 4))
plt.title('Checking BMI for outliers')
sns.boxplot(data['bmi'])

plt.show()

# IQR
# Calculate the upper and lower limits
Q1 = data['bmi'].quantile(0.25)
Q3 = data['bmi'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
# Create arrays of Boolean values indicating the outlier rows
outliers = data[(data['bmi'] < lower) | (data['bmi'] > upper)]
# Removing the outliers
data = data.drop(outliers.index)

plt.figure(figsize=(6, 4))
plt.title('Checking BMI for outliers')
sns.boxplot(data['bmi'])

plt.show()

plt.figure(figsize=(6, 4))
plt.title('Checking blood_glucose_level for outliers')
sns.boxplot(data['blood_glucose_level'])

plt.show()

# IQR
# Calculate the upper and lower limits
Q1 = data['blood_glucose_level'].quantile(0.25)
Q3 = data['blood_glucose_level'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
# Create arrays of Boolean values indicating the outlier rows
outliers = data[(data['blood_glucose_level'] < lower) | (data['blood_glucose_level'] > upper)]
# Removing the outliers
data = data.drop(outliers.index)

plt.figure(figsize=(6, 4))
plt.title('Checking blood_glucose_level for outliers')
sns.boxplot(data['blood_glucose_level'])

plt.show()

plt.figure(figsize=(6, 4))
plt.title('Checking HbA1c_level for outliers')
sns.boxplot(data['HbA1c_level'])

plt.show()

# IQR
# Calculate the upper and lower limits
Q1 = data['HbA1c_level'].quantile(0.25)
Q3 = data['HbA1c_level'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
# Create arrays of Boolean values indicating the outlier rows
outliers = data[(data['HbA1c_level'] < lower) | (data['HbA1c_level'] > upper)]
# Removing the outliers
data = data.drop(outliers.index)

plt.figure(figsize=(6, 4))
plt.title('Checking HbA1c_level for outliers')
sns.boxplot(data['HbA1c_level'])

plt.show()

data['diabetes'].value_counts()

target = data['diabetes']
data = data.drop(columns=['diabetes'], axis=1)

print(data.shape)
print(target.shape)

from sklearn.preprocessing import MinMaxScaler, LabelEncoder
categorical = data[['smoking_history']]
data.drop(columns=[ 'smoking_history'], axis=1, inplace=True)

categorical

categorical = categorical.apply(LabelEncoder().fit_transform)

categorical

data = pd.concat([data, categorical], axis=1)

from imblearn.over_sampling import SMOTE
from collections import Counter
def Smote(data, target):
    counter = Counter(target)
    print('before Oversampling: ', counter)
    smote = SMOTE()
    data, target = smote.fit_resample(data, target)
    counter = Counter(target)
    print('after Oversampling: ', counter)
    return data, target

data, target = Smote(data, target)

data = pd.concat([data, target], axis=1)

data = data.drop(columns=['diabetes'], axis=1)

print(target.shape)
print(data.shape)

data.head()

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(data,target, test_size=0.3,random_state = 42,  shuffle=True)

print(xtrain.shape)
print(ytrain.shape)
print()
print(xtest.shape)
print(ytest.shape)

scaler = MinMaxScaler()
xtrain = pd.DataFrame(scaler.fit_transform(xtrain), columns=xtrain.columns)
xtest = pd.DataFrame(scaler.transform(xtest), columns=xtest.columns)

from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score, RocCurveDisplay, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

def metrics_calculator(y_test, y_pred, model_name):
    '''
    This function calculates all desired performance metrics for a given model.
    '''
    result = pd.DataFrame(data=[accuracy_score(y_test, y_pred),
                                precision_score(y_test, y_pred, average='macro'),
                                recall_score(y_test, y_pred, average='macro'),
                                f1_score(y_test, y_pred, average='macro')],
                          index=['Accuracy','Precision','Recall','F1-score'],
                          columns = [model_name])
    return result



def plot_result(y_pred, model_name) :
    '''
    1) plot Confusion Matrix
    2) plot Classification Report
    '''

    fig, ax = plt.subplots(1, 2, figsize=(15, 4))
    # plt.title(model_name)
    fig.tight_layout()
    #AX left - Confusion Matrix
    cm =confusion_matrix(ytest, y_pred)
    ax[0]=sns.heatmap(cm, cmap='Blues', annot=True, fmt='', linewidths=0.5, ax=ax[0])
    ax[0].set_xlabel('Prediced labels', fontsize=18)
    ax[0].set_ylabel('True labels', fontsize=18)
    ax[0].set_title(f'Confusion Matrix for {model_name} model', fontsize=18)
    # ax[0].xaxis.set_ticklabels(['GAD Positive', 'GAD Negative'])
    # ax[0].yaxis.set_ticklabels(['GAD Positive', 'GAD Negative'])
    #
    # AX Right - Classification Report
    cr = pd.DataFrame(classification_report(ytest, y_pred,output_dict=True)).T
    cr.drop(columns='support', inplace=True)
    ax[1] = sns.heatmap(cr, cmap='Blues', annot=True, fmt='0.3f', linewidths=0.5, ax=ax[1])
    ax[1].xaxis.tick_top()
    ax[1].set_title(f'Classification Report for {model_name} model', fontsize=18)
    plt.show()

parameters = {
    'penalty': ['l1', 'l2', 'elasticnet'],
    'C': [0.001, 0.01, 0.1, 1, 10],
    'class_weight': ['balanced']}

gridSearch = GridSearchCV(estimator=LogisticRegression(), param_grid=parameters, cv=5, scoring='accuracy')
gridSearch.fit(xtrain, ytrain)

Logistic = gridSearch.best_estimator_
Logistic.fit(xtrain, ytrain)

kf1 = KFold(n_splits=5)
cv_scores1 = cross_val_score(Logistic, xtrain, ytrain, cv=kf1, scoring='accuracy')

print('Cv Scores')
print(cv_scores1)
print('Cv Scores Accuracy Mean: ', cv_scores1.mean())

prediction_on_training_data_logistic = Logistic.predict(xtrain)
metrics_calculator(ytrain, prediction_on_training_data_logistic, 'LogisticRegression_OnTraining')

logistic_prediction = Logistic.predict(xtest)
metrics_calculator(ytest, logistic_prediction, 'LogisticRegression_OnTesting')

RandomForest = RandomForestClassifier()
RandomForest.fit(xtrain, ytrain)

kf5 = KFold(n_splits=5)
cv_scores5 = cross_val_score(RandomForest, xtrain, ytrain, cv=kf5, scoring='accuracy')

print('Cv Scores')
print(cv_scores5)
print('Cv Scores Accuracy Mean: ', cv_scores5.mean())

prediction_on_training_data_rf = RandomForest.predict(xtrain)
metrics_calculator_rf_train = metrics_calculator(ytrain, prediction_on_training_data_rf, 'RandomForest_OnTraining')
metrics_calculator_rf_train

rf_prediction = RandomForest.predict(xtest)
metrics_calculator_rf_test = metrics_calculator(ytest, rf_prediction, 'RandomForest_OnTesting')
metrics_calculator_rf_test

